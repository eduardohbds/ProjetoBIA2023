{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ks8zfVYYQL90"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev4anaPUWlgK"
      },
      "source": [
        "## Final Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIMqX_og953s"
      },
      "outputs": [],
      "source": [
        "pip install contractions praw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcBj8akpWoJ6"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import praw\n",
        "import pandas as pd\n",
        "import re\n",
        "from datetime import datetime\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.probability import FreqDist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# NLTK setup\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuAGz6ab9m0f"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "import contractions  # You may need to install this package\n",
        "import pandas as pd\n",
        "# Remove URLs\n",
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub('', text)\n",
        "\n",
        "# Expand Contractions\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "# Remove Special Characters\n",
        "def remove_special_characters(text, remove_digits=False):\n",
        "    pattern = r'[^a-zA-Z\\s]' if not remove_digits else r'[^a-zA-Z0-9\\s]'\n",
        "    return re.sub(pattern, '', text)\n",
        "\n",
        "# Remove Accented Characters\n",
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n",
        "\n",
        "# Full Normalization\n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    text = remove_urls(text)\n",
        "    text = expand_contractions(text)\n",
        "    text = remove_accented_chars(text)\n",
        "    text = remove_special_characters(text, remove_digits=True)\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    return text\n",
        "\n",
        "# Text processing functions\n",
        "def process_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "    return lemmatized_tokens\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "    return sia.polarity_scores(text)['compound']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PM8DY3K5Wr7g"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Reddit API setup and scraping\n",
        "with open(\"./drive/MyDrive/BIA_Analise_de_sentimento/credentials.json\", \"r\") as f:\n",
        "    credentials = json.load(f)\n",
        "\n",
        "client_id = credentials[\"CLIENT_ID\"]\n",
        "secret_key = credentials[\"SECRET_KEY\"]\n",
        "user_agent = \"Scraper 1.0 by /u/eduardo_hbds1\"\n",
        "reddit = praw.Reddit(client_id=client_id, client_secret=secret_key, user_agent=user_agent)\n",
        "\n",
        "posts = set()\n",
        "for submission in reddit.subreddit('webdev').hot(limit=None):\n",
        "    title_and_content = submission.title + \"\\n\" + submission.selftext\n",
        "    posts.add(title_and_content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(posts, columns=['post'])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "MbaBFs3UUuRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvUmOHccW8K0"
      },
      "outputs": [],
      "source": [
        "# DataFrame creation and processing\n",
        "df['normalized_post'] = df['post'].apply(normalize_text)\n",
        "df['processed_post'] = df['normalized_post'].apply(process_text)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXnEuTBHXB1C"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Sentiment Analysis\n",
        "df['sentiment'] = df['normalized_post'].apply(analyze_sentiment)\n",
        "df.to_csv('./drive/MyDrive/BIA_Analise_de_sentimento/headlinesSentiment.csv',encoding='utf-8',index=False)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XemBKgDTD8a5"
      },
      "outputs": [],
      "source": [
        "def classify_sentiment(score):\n",
        "    if score > 0.05:  # Positive sentiment\n",
        "        return 'Positive'\n",
        "    elif score < -0.05:  # Negative sentiment\n",
        "        return 'Negative'\n",
        "    else:  # Neutral sentiment\n",
        "        return 'Neutral'\n",
        "\n",
        "df['sentiment_category'] = df['sentiment'].apply(classify_sentiment)\n",
        "\n",
        "sentiment_counts = df['sentiment_category'].value_counts()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Bar Chart\n",
        "sentiment_counts.plot(kind='bar', color=['green', 'red', 'blue'])\n",
        "plt.title('Sentiment Analysis')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Number of Posts')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Rn5McxRGpDv"
      },
      "outputs": [],
      "source": [
        "def sentiment_category(score):\n",
        "    return 'Positive' if score > 0.05 else 'Negative' if score < -0.05 else 'Neutral'\n",
        "\n",
        "df['sentiment_category'] = df['sentiment'].apply(sentiment_category)\n",
        "\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Optionally remove stop words for more meaningful results\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def get_most_common_words(texts, num_words=10):\n",
        "    words = [word for text in texts for word in text if word not in stop_words]\n",
        "    freq_dist = FreqDist(words)\n",
        "    return freq_dist.most_common(num_words)\n",
        "\n",
        "# Filter the DataFrame for positive and negative texts\n",
        "positive_texts = df[df['sentiment_category'] == 'Positive']['processed_post']\n",
        "negative_texts = df[df['sentiment_category'] == 'Negative']['processed_post']\n",
        "\n",
        "# Get the most common words\n",
        "most_common_positive = get_most_common_words(positive_texts)\n",
        "most_common_negative = get_most_common_words(negative_texts)\n",
        "\n",
        "print(\"Most Common Positive Words:\", most_common_positive)\n",
        "print(\"Most Common Negative Words:\", most_common_negative)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npAKSnXIXER7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Word Frequency Distribution\n",
        "all_words = [word for post in df['processed_post'] for word in post]\n",
        "freq_dist = FreqDist(all_words)\n",
        "df.to_csv('./drive/MyDrive/BIA_Analise_de_sentimento/words.csv',encoding='utf-8',index=False)\n",
        "freq_dist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhrgFnDtXIqW"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Display the most common words\n",
        "most_common_words = freq_dist.most_common(100)\n",
        "print(\"Most common words:\")\n",
        "for word, freq in most_common_words:\n",
        "    print(f\"{word}: {freq}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfE3YoIXErip"
      },
      "outputs": [],
      "source": [
        "freq_dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eICIRUiTXL3S"
      },
      "outputs": [],
      "source": [
        "# Visualization\n",
        "freq_dist.plot(10, cumulative=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHuWbhNwIIZg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('./drive/MyDrive/BIA_Analise_de_sentimento/headlinesSentiment.csv')\n",
        "df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZFKy-T3Mzqi"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('./drive/MyDrive/BIA_Analise_de_sentimento/headlinesSentiment.csv')\n",
        "\n",
        "# Defining a function to classify sentiment\n",
        "def classify_sentiment(score):\n",
        "    if score > 0.05:  # Positive sentiment\n",
        "        return 'Positive'\n",
        "    elif score < -0.05:  # Negative sentiment\n",
        "        return 'Negative'\n",
        "    else:  # Neutral sentiment\n",
        "        return 'Neutral'\n",
        "\n",
        "# Classifying sentiment for each post\n",
        "df['sentiment_category'] = df['sentiment'].apply(classify_sentiment)\n",
        "\n",
        "\n",
        "\n",
        "# Filtering posts by sentiment category\n",
        "positive_posts = df[df['sentiment_category'] == 'Positive']['post']\n",
        "negative_posts = df[df['sentiment_category'] == 'Negative']['post']\n",
        "neutral_posts = df[df['sentiment_category'] == 'Neutral']['post']\n",
        "\n",
        "# Displaying 5 examples of positive posts\n",
        "print(\"Positive Posts:\")\n",
        "for post in positive_posts[:5]:\n",
        "    print(post)\n",
        "    print(\"#########################################################################\")\n",
        "\n",
        "# Displaying 5 examples of negative posts\n",
        "print(\"\\nNegative Posts:\")\n",
        "for post in negative_posts[:5]:\n",
        "    print(post)\n",
        "    print(\"#########################################################################\")\n",
        "\n",
        "\n",
        "# Displaying 5 examples of neutral posts\n",
        "print(\"\\nNeutral Posts:\")\n",
        "for post in neutral_posts[:5]:\n",
        "    print(post)\n",
        "    print(\"#########################################################################\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('./drive/MyDrive/BIA_Analise_de_sentimento/headlinesWithTagSentiment.csv',encoding='utf-8',index=False)"
      ],
      "metadata": {
        "id": "k7BOy4uABnCA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}